\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{xeCJK}
%\usepackage[english]{babel}
\usepackage{float}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    

\begin{document}
\title{Homework 1\\
{\footnotesize}}

\author{\IEEEauthorblockN{葉政義}}
\maketitle


\section{Experiment Results}
建立Multilayer perceptron對Boston housing dataset進行預測；預測目標，自家用的住宅房價中位數(medv)，且對所有的feature進行標準化。Equation 1是cost function,使用Mean Squared Error(MSE)進行模型比較。
\begin{equation}
J(w)=\sum_{i}||\hat{y}^{(i)}-y^{(i)}||^2
\end{equation}

\subsection{The number of hidden layers}
learning rate(lr) = 1e-6，hidden layer = 3 or 5，\\
neurons = 30(in each layer)，activation function = ReLU，p = 0.5，which is dropout rate.
\begin{figure}[H]
\centering{\includegraphics[height=6cm]{Figure_1.png}}
\caption{Cost for 3-hidden layer.}
\label{fig1}

\centering{\includegraphics[height=6cm]{Figure_2.png}}
\caption{MSE for 3-hidden layer.}
\label{fig2}
\end{figure}

\begin{figure}[H]
\centering{\includegraphics[height=6cm]{Figure_3.png}}
\caption{Cost for 5-hidden layer.}
\label{fig3}

\centering{\includegraphics[height=6cm]{Figure_4.png}}
\caption{MSE for 5-hidden layer.}
\label{fig4}
\end{figure}
從Fig. 3和Fig. 4得知，3-hidden layer和5-hidden layer的MSE彼此差不多且都無overfitting的現象，但是 5-hidden layer在第58個到第60個epoch時MSE突然增加，觀察Fig. 3 cost也在這個時候增加，很有可能是在這時候的learning rate太高導致參數更新時跑到cost高的地方。
\subsection{Without Dropout}
learning rate(lr) = 1e-6，hidden layer = 3，\\
neurons = 30(in each layer)，activation function = ReLU，p = 0.5，which is dropout rate.
\begin{figure}[H]
\centering{\includegraphics[height=6cm]{Figure_5.png}}
\caption{Cost for 3-hidden layer without dropout.}
\label{fig5}

\centering{\includegraphics[height=6cm]{Figure_6.png}}
\caption{MSE for 3-hidden layer without dropout.}
\label{fig6}
\end{figure}
不加dropout的模型，cost能降到接近0但是由Fig. 6可以明顯的看到overfitting，也就是training MSE很低但是test MSE變高；除此Fig. 5和Fig. 1相比，不加dropout的模型cost比較平滑；然而加了dropout的模型就比較曲折，原因是在每次計算cost時使用的神經元都不同導致cost有高有低但是整體來看是有逐漸降低的。
\subsection{Learning rate}
learning rate(lr) = 1e-3，hidden layer = 3，\\
neurons = 30(in each layer)，activation function = ReLU，p = 0.5，which is dropout rate.
\begin{figure}[H]
\centering{\includegraphics[height=6cm]{Figure_7.png}}
\caption{Cost for 3-hidden layer，lr = 1e-3.}
\label{fig7}

\centering{\includegraphics[height=6cm]{Figure_9.png}}
\caption{MSE for 3-hidden layer，lr = 1e-3，from 20 epochs to 100 epochs.}
\label{fig9}

\centering{\includegraphics[height=6cm]{Figure_8.png}}
\caption{MSE for 3-hidden layer，lr = 1e-3.}
\label{fig8}
\end{figure}
從Fig. 7和Fig. 1可以得知，當learning rate = 1e-3時，前面幾個epochs的cost就會很大；會這樣子的原因是learning rate太大，有時cost就會在這時候顯示inf 或者 nan。 
\section{Conclusion}
從上面這些圖可以知道對於這筆資料，層數的增加對MSE不會有明顯降低而使用dropout的模型在預測上效果很好，降低了overfitting，learning rate的選擇也很重要，過大的learning rate會讓cost無法降下來。
\end{document}
